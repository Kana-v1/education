Dependability is: availability, reliability, and maintainability.

Dependability attributes:
 - Fault Prevention -> avoid of the occurrence or introduction of faults
 - Fault Tolerance (self-healing, etc) -> avoid service failures in the presence of faults, 
 	обычно добавляют софт для хила и запасные сервисы
 - Fault Removal -> reduce the number and severity of faults
 - Fault Forecasting -> identify the presence, the creation, and the consequences of faults

Dependability procurement - уверенность которую получает возможность системы выполнять функции для которых она(система) предназначена

Варианты поиска косяков в софте: 
 - Статический анализ -> скрипты не запускают программу, но пытаются найти косяки (юнит-тесты, линтеры, etc)
 - Динамический анализ -> aka testing
 
 12 правил веб софта:
 
 1) Codebase 
  Весь прод должен идти с одной и той же кодовой базы. По факту все должны брать и обновлять код из одного и того же репозитория
 
 2) Dependencies
  Тут зависимости имеется в виду модули/пакеты. Т.е. система не должна навернутся от того что версия какого-то там пакета поменялась
  
 3) Configuration
  Конфиги должны быть отделены от кода (короче, используйте env файлы)
  
 4) Backing Services
   Не должно быть разного отношения к своим и 3rd party сервисам
 
 5) Build, Release, Run
  Эти шаги стоит так разделять чтобы можно было повторить каждый отдельный шаг без особых проблем
  
 6) Processes
  Сервисы сами по себе не должны хранить данные (если нужно сохранить данные - это в бд, кеш, etc)
  
 7) Data isolation 
  Один сервис не должен влезать в другой. Для таких целей есть api, rpc, pub-sub
 
 8) Scalability
  Сервисы должны уметь горизонтально масштабироваться
  
 9) Disposability
  Сервисы должны уметь нормально запускаться и останавливаться вне зависимости от причин остановки (если бек падает в панику от того что в бд нет значений это очень так себе)
  
 10) Development/Production parity
  Прод и дев части должны быть максимально одинаковыми (одинаковый код, стек, по хорошему те кто писал код должны сами его деплоить)
  
 11) Logs
  Логи должны лететь в stdout, а не в локальный файл который потом невозможно найти
 
 12) Administrative processes
  Админские цели супер нежелательно делать руками. Т.е. делать миграции написав на проде это все руками затея отнюдь не из лучших 
  
 Scalability - возможность резко увеличить пропускную способность сервисов т.к. держать 20 серверов когда постоянно используется 3 просто дорого, но если есть момент когда нужно 20 - значит должны быть возможность увеличить их количество
 
 resource state - данные которые одинаковые для каждого клиента и которые не взаимодействуют напрямую с клиентом

application state - это данные которые лежат внутри самого приложения (не в бд), такие данные делают приложение не масштабируемым

Durability - данные которые лежат в одном месте (например без реплики) когда-нибудь потеряются.

Cacheability - stateless сервисы идемпотентны, так что их ответы можно положить в кеш и ускорить последующие ответы сервиса

coupling

способы получить tight coupling:
1) самый дефолтный - напрямую лезть в другой сервис
2) сервисы которые должны использовать определенную либу, или определенную версию этой либы
3) ответ от другого сервиса. Сам по себе такой паттерн не плох, но лучше юзать паб-саб для подобных вещей, или хотя бы обрабатывать невалидный ответ от сервиса
4) жестко заданный адрес другого сервиса (будет грустно если бд переедет на другой айпишник и это уложит все сервисы потому что они в миллионе разных мест захардкоили ее айпи)

GraphQL - альтернатива РЕСТу

Remote Procedure Calls (RPC) - позволяет выполнять процедуры в другом неймспейсе (обычно на другом компе). gRPC - одна из реализаций

gRPC: в keyValue.proto все методы это примеры унарного типа, по которым клиент кидает одиночный запрос на сервер и сервер возвращает одиночный ответ. Всего типов 4 и такой - самый простой

Чтобы превратить .proto файл в го ->
    
protoc --go_out=./gRPCtemplate --go_opt=paths=source_relative     --go-grpc_out=./gRPCtemplate --go-grpc_opt=paths=source_relative  keyValue.proto

Plugins

Plugin - main package with >= 1 exported functions and variables that has been built with the -buildmode=plugin flag. It's represented in the plugin package by the Plugin type

Open (функция) - процесс, который загружает плагин в память, валидирует его и ищет заэкспорченые элементы

Symbol (интерфейс) - любая переменная или функция которая экспортится плагин пакетом. 

Lookup(функция) - описывает процес поиска и получения заэкспорченого плагином элемента

Сам плагин создается при помощи подобной команды -> go build -buildmode=plugin -o duck/duck.so duck/duck.go 

при компиляции нужно указывать путь к .so файлу, но сам такой файл должен быть живой только когда его будут дергать (plugin.Open(filePath)), т.е. можно его 3 раза поменять и рантайм будет
динамически меняться

Нет способа вывести все symbols плагина


Hexagonal architecture

Гексагональная архитектура - архитектурный паттерн который использует loose coupling & ioc (inversion of control)

В такой архитектуре кор ничего не знает про то что вне кора и использует его через порты и адаптеры

Элементы:
 The core application - бизнес логика
 
 Ports and adapters - представлены как края гекса. Порты позволяют разным actors взаимодействовать с кор сервисами. Адаптеры "включаются" в порты и передают данные между кором и actors . Например, приложение может иметь "data port", в который включаются адаптеры. Один такой адаптер пишет в бд, а другой - проводит тесты с этими данными.
 
 Actors - что угодно что взаимодействует с кором приложения (юзеры, другие сервисыб etc), или то, с чем взаимодействует кор (бд, брокеры сообщений, etc). В любом случае, они существуют вне гекса.
 
В обычных layered архитектурах каждый слой зависит от нижнего слоя, в гексе кор ничего не знает про внешний мир, адаптеры знают только то, как доставлять информацию в/из кора, внешние адаптеры знают только то, как взаимодействовать с actors

Способы уменьшить нагрузку на сервер:
Полностью отрезать все запросы когда их количество перевалить за определенную величину, или начать их просеивать на более ранней стадии (условно, оставлять 8 из 10 запросов)

Если запрос отвалился и нужно его повторять пока он не будет успешним - лучше чтобы был какой-нибудь бэк лог алгоритм (или просто таймер) чтобы не завалить сервер, но таймер должен быть статическим, а не динамическим, и при этом не простоувеличиваться условно в 2 раза (так он просто откладывает эти запросы и сервак 2 секунды стоит, а потом в шоке) а кидать запросы через рандомные промежутки времени 

Идемпотентность - это про одинаковый результат для 2 одинаковых действий, т.е. если нам 2 раза приходит транзакция с одинаковым ид, то вторую мы просто проигнорим


Для сервиса(контейнера) быть "живым"(healthy) означает что он не упал.

Как вообще может упасть сервис:
 - Ошибка или кончились ресурсы (cpu, memory, database connection, etc)
 - Ошибка у сервиса, от которого есть зависимость (балансирощику, базе данных стало плохо, etc)

Способы проверить живой ли сервис или нет:
 - Liveness check => просто проверить что сервис вообще отвечает. При этом обычно если не отвечает то никто не пытается снова до него достучаться
 
 - Shallow check => то что сверху + пытается убедиться в том, что сервис может функционировать. По большому счету просто проверяет локальные ресурсы. По такой проверке тоже нельзя наверняка сказать что сервис живой
 
 - Deep health => самая глубокая проверка, в которой чекается дополнительно downstream ресурсы (доступ к бд, etc), но такой способ забирает много ресурсов + может быть false positive
 
 -------------
 
 Liveness check
 
 Это просто запрос на эндпоинт. Ответил - живой, нет - значит нет.

 О чем такая НЕ может сказать:
 - может ли сервис слушать свои незанятые порты
 - доступен ли сервис по сети
 - правильно ли настроены конфиги безопасности, фаерволов, банального подключения к бд
 
 Зато дешевый
 
 
 Shallow check 
 
 Что он проверяет:
 - доступность локальных ресурсов (memory, CPU, db connection)
 - возможность читать и писать данные (проверяет место на жестком диске, доступы, и ищет неисправности на уровне железа)
 - проверяет работают ли сапорт процесы (мониторинг, обновление)
 
 
 Deep check
 
 пытается взаимодействовать с соседними к целевому сервисами.
 
 Способ спорный т.к. балансировщик может банально не кидать конкретно на этот под запросы, но некоторые балансировщики умеют бороться с таким "failing open". Если он поймет что все его таргеты застрелились - он кинет трафик на все эти таргеты.
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 









 
 
  

